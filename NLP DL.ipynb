{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "from keras.datasets import mnist\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8),\n",
       "  array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)),\n",
       " (array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8),\n",
       "  array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is a dataset of 60,000 28x28 grayscale images of the 10 digits, \n",
    "#along with a test set of 10,000 images.\n",
    "from keras.datasets import mnist\n",
    "mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BatchDataset' object has no attribute 'make_one_shot_iterator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-7820f74c3079>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0miterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_one_shot_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;31m# Model creation using tensors from the get_next() graph node.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BatchDataset' object has no attribute 'make_one_shot_iterator'"
     ]
    }
   ],
   "source": [
    "if K.backend() != 'tensorflow':\n",
    "    raise RuntimeError('This example can only run with the TensorFlow backend,'\n",
    "                       ' because it requires the Datset API, which is not'\n",
    "                       ' supported on other platforms.')\n",
    "\n",
    "\n",
    "def cnn_layers(inputs):\n",
    "    x = layers.Conv2D(32, (3, 3),\n",
    "                      activation='relu', padding='valid')(inputs)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    predictions = layers.Dense(num_classes,\n",
    "                               activation='softmax',\n",
    "                               name='x_train_out')(x)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "buffer_size = 10000\n",
    "steps_per_epoch = int(np.ceil(60000 / float(batch_size)))  # = 469\n",
    "epochs = 5\n",
    "num_classes = 10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype(np.float32) / 255\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "y_train = tf.one_hot(y_train, num_classes)\n",
    "\n",
    "# Create the dataset and its associated one-shot iterator.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "dataset = dataset.repeat()\n",
    "dataset = dataset.shuffle(buffer_size)\n",
    "dataset = dataset.batch(batch_size)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "# Model creation using tensors from the get_next() graph node.\n",
    "inputs, targets = iterator.get_next()\n",
    "model_input = layers.Input(tensor=inputs)\n",
    "model_output = cnn_layers(model_input)\n",
    "train_model = keras.models.Model(inputs=model_input, outputs=model_output)\n",
    "\n",
    "train_model.compile(optimizer=keras.optimizers.RMSprop(lr=2e-3, decay=1e-5),\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'],\n",
    "                    target_tensors=[targets])\n",
    "train_model.summary()\n",
    "\n",
    "train_model.fit(epochs=epochs,\n",
    "                steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "# Save the model weights.\n",
    "weight_path = os.path.join(tempfile.gettempdir(), 'saved_wt.h5')\n",
    "train_model.save_weights(weight_path)\n",
    "\n",
    "# Clean up the TF session.\n",
    "K.clear_session()\n",
    "\n",
    "# Second session to test loading trained model without tensors.\n",
    "x_test = x_test.astype(np.float32)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "x_test_inp = layers.Input(shape=x_test.shape[1:])\n",
    "test_out = cnn_layers(x_test_inp)\n",
    "test_model = keras.models.Model(inputs=x_test_inp, outputs=test_out)\n",
    "\n",
    "test_model.load_weights(weight_path)\n",
    "test_model.compile(optimizer='rmsprop',\n",
    "                   loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "test_model.summary()\n",
    "\n",
    "loss, acc = test_model.evaluate(x_test, y_test, num_classes)\n",
    "print('\\nTest accuracy: {0}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------Practice------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://keras.io/examples/mnist_dataset_api/\n",
    "Padding:\n",
    "#https://www.pyimagesearch.com/2018/12/31/keras-conv2d-and-convolutional-layers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = layers.Conv2D(32, (3, 3),activation='relu', padding='valid')(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conv2D is a 2D Convolution Layer, \n",
    "this layer creates a convolution kernel that is wind with layers input which helps produce a tensor of outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a total of 32  filters. \n",
    "Max pooling is then used to reduce the spatial dimensions of the output volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel: In image processing kernel is a convolution matrix or masks \n",
    "        which can be used for blurring, sharpening, embossing, edge detection, \n",
    "        and more by doing a convolution between a kernel and an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel_size  must be an odd integer as well.\n",
    "\n",
    "Typical values for kernel_size  include: (1, 1) , (3, 3) , (5, 5) , (7, 7) .\n",
    "It’s rare to see kernel sizes larger than 7×7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 3×3 kernel applied to an image with padding. \n",
    "The Keras Conv2D padding parameter accepts either \"valid\" (no padding) or \"same\" (padding + preserving spatial dimensions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the default Keras Conv2D value is valid  I will typically set it to same  \n",
    "for the majority of the layers in my network and then either reduce spatial dimensions of my volume by either:\n",
    "\n",
    "Max pooling\n",
    "Strided convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
