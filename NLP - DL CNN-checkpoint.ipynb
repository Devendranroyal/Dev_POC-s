{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "from keras.datasets import mnist\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 14s 1us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8),\n",
       "  array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)),\n",
       " (array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8),\n",
       "  array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-34cb32fd40c4>:50: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "WARNING:tensorflow:From C:\\Users\\Devendran.D\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\Devendran.D\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               819712    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "x_train_out (Dense)          (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 843,658\n",
      "Trainable params: 843,658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\Devendran.D\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/5\n",
      "469/469 [==============================] - 70s 149ms/step - loss: 0.1680 - accuracy: 0.9491\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 61s 130ms/step - loss: 0.0498 - accuracy: 0.9852\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 63s 134ms/step - loss: 0.0362 - accuracy: 0.9885\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 63s 135ms/step - loss: 0.0305 - accuracy: 0.9909\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 59s 126ms/step - loss: 0.0262 - accuracy: 0.9920\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               819712    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "x_train_out (Dense)          (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 843,658\n",
      "Trainable params: 843,658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10000/10000 [==============================] - 8s 759us/step\n",
      "\n",
      "Test accuracy: 0.9593999981880188\n"
     ]
    }
   ],
   "source": [
    "if K.backend() != 'tensorflow':\n",
    "    raise RuntimeError('This example can only run with the TensorFlow backend,'\n",
    "                       ' because it requires the Datset API, which is not'\n",
    "                       ' supported on other platforms.')\n",
    "\n",
    "\n",
    "def cnn_layers(inputs):\n",
    "    x = layers.Conv2D(32, (3, 3),\n",
    "                      activation='relu', padding='valid')(inputs)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    predictions = layers.Dense(num_classes,\n",
    "                               activation='softmax',\n",
    "                               name='x_train_out')(x)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "buffer_size = 10000\n",
    "steps_per_epoch = int(np.ceil(60000 / float(batch_size)))  # = 469\n",
    "epochs = 5\n",
    "num_classes = 10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype(np.float32) / 255\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "y_train = tf.one_hot(y_train, num_classes)\n",
    "\n",
    "# Create the dataset and its associated one-shot iterator.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "dataset = dataset.repeat()\n",
    "dataset = dataset.shuffle(buffer_size)\n",
    "dataset = dataset.batch(batch_size)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "# Model creation using tensors from the get_next() graph node.\n",
    "inputs, targets = iterator.get_next()\n",
    "model_input = layers.Input(tensor=inputs)\n",
    "model_output = cnn_layers(model_input)\n",
    "train_model = keras.models.Model(inputs=model_input, outputs=model_output)\n",
    "\n",
    "train_model.compile(optimizer=keras.optimizers.RMSprop(lr=2e-3, decay=1e-5),\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'],\n",
    "                    target_tensors=[targets])\n",
    "train_model.summary()\n",
    "\n",
    "train_model.fit(epochs=epochs,\n",
    "                steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "# Save the model weights.\n",
    "weight_path = os.path.join(tempfile.gettempdir(), 'saved_wt.h5')\n",
    "train_model.save_weights(weight_path)\n",
    "\n",
    "# Clean up the TF session.\n",
    "K.clear_session()\n",
    "\n",
    "# Second session to test loading trained model without tensors.\n",
    "x_test = x_test.astype(np.float32)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "x_test_inp = layers.Input(shape=x_test.shape[1:])\n",
    "test_out = cnn_layers(x_test_inp)\n",
    "test_model = keras.models.Model(inputs=x_test_inp, outputs=test_out)\n",
    "\n",
    "test_model.load_weights(weight_path)\n",
    "test_model.compile(optimizer='rmsprop',\n",
    "                   loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "test_model.summary()\n",
    "\n",
    "loss, acc = test_model.evaluate(x_test, y_test, num_classes)\n",
    "print('\\nTest accuracy: {0}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------Practice------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://keras.io/examples/mnist_dataset_api/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if K.backend() != 'tensorflow':\n",
    "    raise RuntimeError('This example can only run with the TensorFlow backend,'\n",
    "                       ' because it requires the Datset API, which is not'\n",
    "                       ' supported on other platforms.')\n",
    "\n",
    "\n",
    "def cnn_layers(inputs):\n",
    "    x = layers.Conv2D(32, kernel_size =(3, 3),\n",
    "                      activation='relu', padding='valid')(inputs)\n",
    "    #Keras Conv2D is a 2D Convolution Layer, \n",
    "    #A convolutional layer that extracts features from a source image.\n",
    "    #this layer creates a convolution kernel that is wind with layers input which helps produce a tensor of outputs.\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    # Max pooling is then used to reduce the spatial dimensions of the output volume.\n",
    "    #A pooling layer that downsamples each feature to reduce its dimensionality and focus on the most important elements.\n",
    "    \n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "     #512 in the hidden layer and 10 neurons=num_classes in the output layer. \n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    predictions = layers.Dense(num_classes,\n",
    "                               activation='softmax',\n",
    "                               name='x_train_out')(x)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "batch_size = 128 \n",
    "#The number of steps that are required to complete an epoch is ceil(dataset size/batch size). \n",
    "buffer_size = 10000\n",
    "#if I use the follwing code with 'dataset = dataset.shuffle(buffer_size=2325000) '\n",
    "#the cost of time to load images is too long for me.\n",
    "\n",
    "\n",
    "steps_per_epoch = int(np.ceil(60000 / float(batch_size)))  # = 469\n",
    "#Total number of steps (batches of samples) to yield from generator before declaring one epoch finished and starting the next epoch. \n",
    "#It should typically be equal to the number of unique samples of your dataset divided by the batch size.\n",
    "\n",
    "epochs = 5\n",
    "#an epoch is a full pass of the dataset. \n",
    "#The epoch simply indicates how many times the dataset has been fed to the network.\n",
    "\n",
    "num_classes = 10\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype(np.float32) / 255\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "y_train = tf.one_hot(y_train, num_classes)\n",
    "\n",
    "# Create the dataset and its associated one-shot iterator.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "dataset = dataset.repeat()\n",
    "dataset = dataset.shuffle(buffer_size)\n",
    "dataset = dataset.batch(batch_size)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "# Model creation using tensors from the get_next() graph node.\n",
    "inputs, targets = iterator.get_next()\n",
    "model_input = layers.Input(tensor=inputs)\n",
    "model_output = cnn_layers(model_input)\n",
    "train_model = keras.models.Model(inputs=model_input, outputs=model_output)\n",
    "\n",
    "train_model.compile(optimizer=keras.optimizers.RMSprop(lr=2e-3, decay=1e-5),\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'],\n",
    "                    target_tensors=[targets])\n",
    "train_model.summary()\n",
    "\n",
    "train_model.fit(epochs=epochs,\n",
    "                steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "# Save the model weights.\n",
    "weight_path = os.path.join(tempfile.gettempdir(), 'saved_wt.h5')\n",
    "train_model.save_weights(weight_path)\n",
    "\n",
    "# Clean up the TF session.\n",
    "K.clear_session()\n",
    "\n",
    "# Second session to test loading trained model without tensors.\n",
    "x_test = x_test.astype(np.float32)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "x_test_inp = layers.Input(shape=x_test.shape[1:])\n",
    "test_out = cnn_layers(x_test_inp)\n",
    "test_model = keras.models.Model(inputs=x_test_inp, outputs=test_out)\n",
    "\n",
    "test_model.load_weights(weight_path)\n",
    "test_model.compile(optimizer='rmsprop',\n",
    "                   loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "test_model.summary()\n",
    "\n",
    "loss, acc = test_model.evaluate(x_test, y_test, num_classes)\n",
    "print('\\nTest accuracy: {0}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the important arguments of the tf.layers.conv2d() abstraction:\n",
    "\n",
    "● Inputs – a Tensor input, representing image pixels which should have been reshaped into a 2D format\n",
    "\n",
    "● filters – the number of filters in the convolution (dimensionality of the output space).\n",
    "\n",
    "● kernel_size – the filter size, an integer or tuple of 2 integers, specifying the height and width of the convolution window. Set a single integer to use a filter with identical height and width.\n",
    "\n",
    "● strides – an integer or tuple of 2 integers, specifying how the filter should move along the height and width. Set a single integer to use the same stride value for both dimensions.\n",
    "\n",
    "● padding: \"VALID\", meaning the image is padded with a border of one pixel, or \"SAME\", meaning the filter moves within the image with no padding, generating a smaller output.\n",
    "\n",
    "● data_format – specifies ordering of dimensions in the inputs, can be either channels_last (default, inputs with shape [batch, height, width, channels]) or channels_first (inputs with shape [batch, channels, height, width].\n",
    "\n",
    "● dilation_rate – enables advanced convolutional structures with dilated (expanded) convolutions. Use an integer or tuple of 2 integers to specify the dilation rate.\n",
    "\n",
    "● activation – the activation function you’d like to use. Set to None for linear activation.\n",
    "\n",
    "● use_bias – boolean, specifies whether a bias should be added to the layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conv2D is a 2D Convolution Layer, this layer creates a convolution kernel that is wind with layers input which helps produce a tensor of outputs.\n",
    "\n",
    "a total of 32 filters. Max pooling is then used to reduce the spatial dimensions of the output volume.\n",
    "\n",
    "Kernel: In image processing kernel is a convolution matrix or masks which can be used for blurring, sharpening, embossing, edge detection, and more by doing a convolution between a kernel and an image.\n",
    "\n",
    "The kernel_size must be an odd integer as well.\n",
    "\n",
    "Typical values for kernel_size include: (1, 1) , (3, 3) , (5, 5) , (7, 7) . It’s rare to see kernel sizes larger than 7×7.\n",
    "\n",
    "ReLU layer(activation='relu'): ReLU is the abbreviation of rectified linear unit, which applies the non-saturating activation function {\\textstyle f(x)=\\max(0,x)}{\\textstyle f(x)=\\max(0,x)}.[54] It effectively removes negative values from an activation map by setting them to zero.[64] It increases the nonlinear properties of the decision function and of the overall network without affecting the receptive fields of the convolution layer.\n",
    "\n",
    "Padding:(padding='valid'or'same') A 3×3 kernel applied to an image with padding. The Keras Conv2D padding parameter accepts either \"valid\" (no padding) or \"same\" (padding + preserving spatial dimensions).\n",
    "\n",
    "While the default Keras Conv2D value is valid I will typically set it to same for the majority of the layers in my network and then either reduce spatial dimensions of my volume by either:\n",
    "\n",
    "Max pooling Strided convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "role of the Flatten layer:\n",
    "\n",
    "A flatten operation on a tensor reshapes the tensor to have the shape that is equal to the number of elements contained in tensor non including the batch dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps_per_epoch give you the chance to feeding process of the NN when updating the learning rate using ReduceLROnPlateuau(). This callback checks the drop of the loss once each epoch has finished and if the loss has stagnated for a patience number of consecutive epochs, the callback decreases the learning rate to \"slow-cook\" the network. If your dataset is huge, as it is usually the case when you need to use generators, ou would probably like to decay the learning rate within a single epoch (since it includes a big number of data). This can be achieved by setting steps_per_epoch to a value that is less than n_samples // batch_size without affecting the overall number of training epochs of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By definition, an epoch is a full pass of the dataset. The number of steps that are required to complete an epoch is ceil(dataset size/batch size). At each step, #(batch size) samples are fed to the network, a mean loss is calculated and the weights are updated based on that loss. So at each step weights are updated once. The epoch simply indicates how many times the dataset has been fed to the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The padding parameter has two values: valid or same. \n",
    "    Valid means the input is not zero-padded, so the output of the convolution will be smaller than the dimensions of the original image. \n",
    "    Same means the input will be zero-padded, so the convolution output can be the same size as the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stride is the speed by which the filter moves across the image, or the number of pixels it shifts every time. The stride is defined as a 4D tensor, \n",
    "because the input has four dimensions: [number_of_samples, height, width, colour_channels]. \n",
    "Setting the strides tensor to [1, strides, strides, 1] applies the filter to every image, every color channel, and every image patch in the height and width dimensions. \n",
    "1 at the beginning and end specifies you won’t skip an image or an entire color channel. \n",
    "For example, strides=[1, 2, 2, 1] applies the filter to half the image patches in each dimension, and skips half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
