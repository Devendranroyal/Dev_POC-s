{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import pi\n",
    "from bokeh.palettes import Spectral6, Category20c,Category10,Accent,Category20\n",
    "from bokeh.palettes import Greys256,Inferno256,Magma256,Plasma256,Viridis256,Cividis256,Turbo256,linear_palette,Set3,Pastel1,Spectral5,Spectral6\n",
    "from bokeh.io import show, output_file, output_notebook, push_notebook\n",
    "from bokeh.layouts import row, column, gridplot\n",
    "from bokeh.models import CustomJS, Slider\n",
    "from bokeh.models.widgets import Div\n",
    "from bokeh.transform import factor_cmap, cumsum\n",
    "from bokeh.models.widgets import Paragraph,DataTable, TableColumn\n",
    "from bokeh.models import ColumnDataSource,HoverTool\n",
    "from bokeh.plotting import figure, output_file, show, ColumnDataSource\n",
    "from bokeh.models.layouts import LayoutDOM, Box, Row, Column, GridBox, Spacer, WidgetBox\n",
    "from bokeh.layouts import widgetbox, column, row\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk import tokenize \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "df=pd.read_csv('train.tsv',delimiter='\\t',encoding='utf-8')\n",
    "df1=pd.read_csv('test.tsv',delimiter='\\t',encoding='utf-8')\n",
    "df.dropna(inplace=True)\n",
    "def LableFunc(Sentiment):\n",
    "    if Sentiment>=3:\n",
    "        return 'Positive'\n",
    "    elif Sentiment<=1:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "df['Label']=df['Sentiment'].apply(LableFunc)\n",
    "\n",
    "stop_words=set(stopwords.words('english'))\n",
    "lemma=WordNetLemmatizer()\n",
    "\n",
    "def clean_Phrase(phrase_text):\n",
    "    phrase_text=re.sub(r'http\\S+','',phrase_text)                             #removing the url\n",
    "    phrase_text=re.sub('[^a-zA-Z]','',phrase_text)                            #removing Numbers and punctuatino\n",
    "    phrase_text=str(phrase_text).lower()                                      #Convert all characters into lowercase\n",
    "    phrase_text=word_tokenize(phrase_text)                                    #Tokenization\n",
    "    phrase_text=[item for item in phrase_text if item not in stop_words]      #Removing Stop Words\n",
    "    phrase_text=[lemma.lemmatize(word=w,pos='v') for w in phrase_text]        #Lemmatization\n",
    "    phrase_text=[i for i in phrase_text if len(i)>2]                          #Remove the words havig length <=2\n",
    "    phrase_text=''.join(phrase_text)                                          #Converting list to string\n",
    "    return phrase_text\n",
    "\n",
    "df['clean_Phrase']=df['Phrase'].apply(clean_Phrase)\n",
    "#df1['clean_Phrase']=df1['Phrase'].apply(clean_Phrase)\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "text = df['clean_Phrase'].values\n",
    "words = '' \n",
    "for val in df['clean_Phrase']:   \n",
    "     val = str(val) \n",
    "     tokens = val.split('_')  \n",
    "     words += \" \".join(tokens)+\" \"\n",
    "wordcloud = WordCloud(\n",
    "    width = 300,\n",
    "    height = 300,\n",
    "    background_color = 'white',\n",
    "    min_font_size=4,\n",
    "    stopwords = STOPWORDS).generate(str(words))\n",
    "fig = plt.figure(\n",
    "    figsize = (4, 4),\n",
    "    facecolor=None)\n",
    "plt.imshow(wordcloud, interpolation = 'spline36')\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "#plt.show()\n",
    "\n",
    "wordcloud.to_file(\"wc.png\")\n",
    "\n",
    "output_file(\"NLP_Dashboard_slider.html\")\n",
    "tools=\"pan,wheel_zoom,box_zoom,reset,save,box_select,hover\"\n",
    "\n",
    "x = Counter({\n",
    "    'train': 156060,\n",
    "    'test': 66292,\n",
    "    'dev': 0\n",
    "})\n",
    "d = pd.DataFrame.from_dict(dict(x), orient='index').reset_index().rename(index=str, columns={0:'value', 'index':'data'})\n",
    "d['percent'] = d['value'] / sum(x.values()) * 100\n",
    "d['angle'] = d['value'] / sum(x.values()) * 2*pi\n",
    "d['color'] = Accent[len(x)]\n",
    "#z=110*(data['value']/data['value'].sum())\n",
    "p2 = figure(plot_height=400,plot_width=400, title=\"Count of train and test\",\n",
    "           tools=\"hover\", tooltips=\"@data:@value\", x_range=(-0.5, 0.5))\n",
    "p2.annular_wedge(x=0, y=1, inner_radius=0.10, outer_radius=0.25, direction=\"anticlock\",\n",
    "        start_angle=cumsum('angle', include_zero=True), end_angle=cumsum('angle'),\n",
    "        line_color=\"white\", fill_color='color', source=d)\n",
    "\n",
    "#labels = LabelSet(x=0, y=1, text='value',\n",
    "        #angle=cumsum('angle', include_zero=True), source=source, render_mode='canvas')\n",
    "\n",
    "#p2.add_layout(labels)\n",
    "p2.axis.axis_label=None\n",
    "p2.axis.visible=False\n",
    "p2.grid.grid_line_color = None\n",
    "\n",
    "x=df['Sentiment'].value_counts()\n",
    "data = pd.Series(x).reset_index(name='Count').rename(columns={'index':'Sentiment'})\n",
    "#p4 = figure(tools=\"pan,wheel_zoom,box_zoom,reset\")\n",
    "p4 = figure(plot_width=400, plot_height=400,x_axis_label=\"Count of each sentiments\", y_axis_label=\"Count\",title=\"Count of Each Sentiments\", tools=tools,toolbar_location=\"right\" ,tooltips=[(\"Sentiment\",\"@x\"),(\"Count\",\"@top\")])\n",
    "p4.vbar(x=range(5), top=data['Count'], width=0.7,color=Spectral5,line_color=\"black\")\n",
    "p4.y_range.start =0\n",
    "#p4.y_range.end = 100000\n",
    "#p4.xaxis.major_label_orientation = pi/4\n",
    "p4.grid.grid_line_color = None\n",
    "\n",
    "p3 = figure(plot_width=400, plot_height=400, title=\"WORDCOUNT\")\n",
    "p3.image_url(url=['wc.png'], x=0, y=0, w=0.8, h=0.8,anchor=\"bottom_left\")\n",
    "p3.xaxis.visible = None\n",
    "p3.yaxis.visible = None\n",
    "\n",
    "\n",
    "x=df['Sentiment'].value_counts()\n",
    "data = pd.Series(x).reset_index(name='Count').rename(columns={'index':'Sentiment'})\n",
    "#data = pd.Series(x).reset_index(name='value').rename(columns={'index':'country'})\n",
    "data['angle'] = data['Count']/data['Count'].sum() * 2*pi\n",
    "data['percent'] = data['Count'] /data['Count'].sum() * 100\n",
    "data['color'] = Category20[len(x)]\n",
    "p5 = figure(plot_height=400,plot_width=400, title=\"Percentage of each sentiments\",\n",
    "           tools=\"hover\", tooltips=\"@Sentiment: @percent{0.2f} %\", x_range=(-0.5, 1.0))\n",
    "p5.wedge(x=0.3, y=1, radius=0.4,\n",
    "        start_angle=cumsum('angle', include_zero=True), end_angle=cumsum('angle'),\n",
    "        line_color=\"white\", fill_color='color', source=data)\n",
    "p5.axis.axis_label=None\n",
    "p5.axis.visible=False\n",
    "p5.grid.grid_line_color = None\n",
    "\n",
    "df0=df[(df.Sentiment ==0)]\n",
    "df0.sort_values(by=['Phrase'], inplace=True,ascending=False)\n",
    "#df0.head(10)\n",
    "x=df0['clean_Phrase'].value_counts()\n",
    "data0 = pd.Series(x).reset_index(name='Count').rename(columns={'index':'Sentiment0'})\n",
    "df0=data0.head(30)\n",
    "#df0.sort_values(by=['Count'], inplace=True,ascending=False)\n",
    "\n",
    "df1=df[(df.Sentiment ==1)]\n",
    "df1.sort_values(by=['Phrase'], inplace=True,ascending=False)\n",
    "#df1.head(10)\n",
    "x=df1['clean_Phrase'].value_counts()\n",
    "data1 = pd.Series(x).reset_index(name='Count').rename(columns={'index':'Sentiment1'})\n",
    "df1=data1.head(30)\n",
    "#df0.sort_values(by=['Count'], inplace=True,ascending=False)\n",
    "\n",
    "df2=df[(df.Sentiment ==2)]\n",
    "df2.sort_values(by=['Phrase'], inplace=True,ascending=False)\n",
    "#df2.head(10)\n",
    "x=df2['clean_Phrase'].value_counts()\n",
    "data2 = pd.Series(x).reset_index(name='Count').rename(columns={'index':'Sentiment2'})\n",
    "df2=data2.head(30)\n",
    "#df0.sort_values(by=['Count'], inplace=True,ascending=False)\n",
    "\n",
    "df3=df[(df.Sentiment ==3)]\n",
    "df3.sort_values(by=['Phrase'], inplace=True,ascending=False)\n",
    "#df3.head(10)\n",
    "x=df3['clean_Phrase'].value_counts()\n",
    "data3 = pd.Series(x).reset_index(name='Count').rename(columns={'index':'Sentiment3'})\n",
    "df3=data3.head(30)\n",
    "#df0.sort_values(by=['Count'], inplace=True,ascending=False)\n",
    "\n",
    "df4=df[(df.Sentiment ==4)]\n",
    "df4.sort_values(by=['Phrase'], inplace=True,ascending=False)\n",
    "#df4.head(10)\n",
    "x=df4['clean_Phrase'].value_counts()\n",
    "data4 = pd.Series(x).reset_index(name='Count').rename(columns={'index':'Sentiment4'})\n",
    "df4=data4.head(30)\n",
    "#df0.sort_values(by=['Count'], inplace=True,ascending=False)\n",
    "\n",
    "def changeArea(attr, old, new):\n",
    "    a=slider.value\n",
    "    b=slider1.value\n",
    "    if(a==0):\n",
    "    \tsource.data = df0.head(b)\n",
    "    if(a==1):\n",
    "    \tsource.data = df1.head(b)\n",
    "    if(a==2):\n",
    "    \tsource.data = df2.head(b)\n",
    "    if(a==3):\n",
    "    \tsource.data = df3.head(b)\n",
    "    if(a==4):\n",
    "    \tsource.data = df4.head(b)\n",
    "\n",
    "source = ColumnDataSource(data=df0.head(5))\n",
    "columns = [\n",
    "    TableColumn(field=\"clean_Phrase\", title='CleanPhrase'),\n",
    "    TableColumn(field=\"Count\", title='Count')\n",
    "    ]\n",
    "p6= DataTable(source=source, columns=columns, width=600, height=400)\n",
    "slider = Slider(start=0, end=4, value=0, step=1, title=\"Sentiment Slider\",bar_color=\"green\")\n",
    "slider.on_change('value', changeArea)\n",
    "slider1 = Slider(start=5, end=30, value=5, step=5, title=\"Top Slider\",bar_color=\"red\")\n",
    "slider1.on_change('value', changeArea)\n",
    "\n",
    "pre=Div(text=\"\"\" <div><h3><strong><center> Sentimental Analysis</center></strong><h3></div>\"\"\",\n",
    "        align='center',style={'color':'firebrick','font-size':'25px','font-family':'Helvetica'})\n",
    "col1 = row(p2,p4,p3)\n",
    "col2 = row(slider,slider1)\n",
    "col3 = row(p6,p5)\n",
    "from bokeh.io import curdoc\n",
    "#layout = column(widgetbox(slider,slider1), p6)\n",
    "layout=column(pre,col1,col2,col3)\n",
    "curdoc().add_root(layout)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
